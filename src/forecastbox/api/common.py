"""
Pydantic models for interchanges between ui, controller and worker
"""

# TODO split into submodules

from pydantic import BaseModel, Field
from enum import Enum
import datetime as dt
from typing import Optional, Any, Callable, cast
from typing_extensions import Self
from base64 import b64encode, b64decode
import cloudpickle


class JobTemplateExample(str, Enum):
	"""Job Catalog"""

	# NOTE this will get eventually replaced by external import/lookup system
	# See plugin/lookup.py

	hello_world = "hello_world"
	hello_torch = "hello_torch"
	hello_image = "hello_image"
	hello_tasks = "hello_tasks"
	hello_earth = "hello_earth"
	hello_aifsl = "hello_aifsl"
	temperature_nbeats = "temperature_nbeats"


class JinjaTemplate(str, Enum):
	job = "job.html"
	main = "index.html"  # index is existing enum field
	prepare = "prepare.html"
	aifs = "aifs.html"
	caco_result = "caco_result.html"


class RegisteredTask(str, Enum):
	"""Job Catalog"""

	# NOTE this will get eventually replaced by external import/lookup system
	# See plugin/lookup.py

	# data sources
	mars_oper_sfc_box = "mars_oper_sfc_box"
	mars_enfo_range_temp = "mars_enfo_range_temp"
	create_numpy_array = "create_numpy_array"
	aifs_fetch_and_predict = "aifs_fetch_and_predict"

	# data sinks
	grib_to_file = "grib_to_file"
	plot_single_grib = "plot_single_grib"
	display_numpy_array = "display_numpy_array"
	nbeats_predict = "nbeats_predict"

	# postproc
	grib_mir = "grib_mir"

	# hybrids
	hello_world = "hello_world"
	hello_torch = "hello_torch"
	hello_image = "hello_image"


class DatasetId(BaseModel):
	dataset_id: str


class TaskParameter(BaseModel):
	# NOTE this would ideally be a pydantic class
	# that would however introduce a requirement for jobs not to be text but bytecode
	# but we may want to do it anyway because we'd need custom validations, esp for the rich classes etc
	# Or we could introduce custom subtypes like lat, lon, latLonBox, marsParam, ...
	clazz: str  # see api.type_system on whats supported
	default: str = ""  # always string because we put it to html form... will be deserd via type_name


class TaskEnvironment(BaseModel):
	packages: list[str] = Field(description="module names + optionally versions, in the pip format", default_factory=list)

	def __add__(self, other: Self) -> Self:
		# consider using pyrsistent here
		return self.__class__(packages=self.packages + other.packages)


class TaskDefinition(BaseModel):
	"""Used for generating input forms and parameter validation"""

	entrypoint: str = Field(description="python_module.function_name")
	user_params: dict[str, TaskParameter]
	output_class: str  # used for serde
	dynamic_param_classes: dict[str, str] = Field(default_factory=dict)
	environment: TaskEnvironment = Field(default_factory=TaskEnvironment)

	def signature_repr(self) -> str:
		dparams = ",".join(self.dynamic_param_classes.values())
		return f"({dparams}) -> {self.output_class}"


class Task(BaseModel):
	"""Represents an atomic computation done in a single process.
	Created from user's input (validated via TaskDefinition)"""

	name: str  # name of the task within the DAG
	static_params_kw: dict[str, Any]  # name, value
	static_params_ps: dict[int, Any]  # position, value
	dataset_inputs_ps: dict[int, DatasetId]
	dataset_inputs_kw: dict[str, DatasetId]
	classes_inputs_kw: dict[str, str]
	classes_inputs_ps: dict[int, str]
	entrypoint: Optional[str] = Field(description="python_module.submodules.function_name")
	func: Optional[str] = Field(None, description="b64 cloud-pickled Callable. Prefered over `entrypoint` if given")
	output_name: Optional[DatasetId]
	output_class: str  # used for serde
	environment: TaskEnvironment

	@staticmethod
	def func_dec(f: str) -> Callable:
		return cast(Callable, cloudpickle.loads(b64decode(f)))

	@staticmethod
	def func_enc(f: Callable) -> str:
		return b64encode(cloudpickle.dumps(f)).decode("ascii")


class TaskDAG(BaseModel):
	"""Represents a complete (distributed) computation, consisting of atomic Tasks.
	Needs no further user input, finishes with the output that the user specified."""

	tasks: list[Task]  # assumed to be in topological (ie, computable) order -- eg, schedule
	output_id: Optional[DatasetId]
	# TODO add in free(dataset_id) events into the tasks
	# TODO add some mechanism for freeing the output_name(dataset_id) as well


class TaskDAGBuilder(BaseModel):
	"""Used to build html form for parameter inputting, and then together with that the TaskDAG itself"""

	tasks: list[tuple[str, TaskDefinition]]  # NOTE already assumed to be in (some) topological order
	dynamic_task_inputs: dict[str, dict[str, str]]  # task_name: {param_name: data_source}
	final_output_at: str


class JobId(BaseModel):
	job_id: str


class JobStatusEnum(str, Enum):
	submitted = "submitted"
	assigned = "assigned"
	preparing = "preparing"
	running = "running"
	failed = "failed"
	finished = "finished"

	@classmethod
	def valid_transition(cls, before: Optional[Self], after: Self) -> bool:
		# maybe mixin int (for ordinality), but be careful about pydantic serde
		lookup = list(cls)  # maybe cache
		return before is None or lookup.index(before) < lookup.index(after)


class JobStatus(BaseModel):
	job_id: JobId
	created_at: dt.datetime
	updated_at: dt.datetime
	status: JobStatusEnum
	status_detail: str
	stages: dict[str, JobStatusEnum] = Field(default_factory=dict)
	result: Optional[str] = Field(description="URL where the result can be streamed from")


class JobStatusUpdate(BaseModel):
	job_id: JobId
	status: JobStatusEnum
	task_name: Optional[str] = None
	result: Optional[str] = None
	status_detail: Optional[str] = None


# controller: workers
class WorkerId(BaseModel):
	worker_id: str


class WorkerRegistration(BaseModel):
	url_base64: str
	memory_mb: int

	@classmethod
	def from_raw(cls, url: str, memory_mb: int) -> Self:
		return cls(url_base64=b64encode(url.encode()).decode(), memory_mb=memory_mb)

	def url_raw(self) -> str:
		return b64decode(self.url_base64.encode()).decode()
